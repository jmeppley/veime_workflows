"""
Given either:

 * fasta of fragments from concatemeric reads
 * directory of directories with clusterd fragments in fasta: {fragment_dir}/{cluster_name}/all.fasta
 
Polish PR fragments in each read or cluster by:

 * pick represetatve as fragment with highest mean %ANI to all others
 * polish rep with others using racon (3 passes)
 * polish with others using medaka
 
Additionally, predict genes on all the drafts and compare gene lengths at each step.

NOTES:

If you ran Sankefile.frags and use the same workging directory, this workflow wil find the clustered 
fragments and use those. 

To ignore the clustering (and to just use fragments from the same read for 
polishing) set fragments_dir to an empty (or missing) directory and set fragments_fasta to 
{working_dir}/fragments.fasta. The fragments are expected to be named with the read name and 
fragment number joined by a colon (":"). The workflow recovers the read name from the fragment name
by removing whaterver matches the regular expression: r':\d+$'. You can change the expression 
with the "fragment_pattern" configuration value.

"""

from Bio import SeqIO


#####
# Configuration
working_dir = config.get('working_dir', 'test/run')

# empty list for no input ({fragments_dir} must exist if no {fragment_fasta} given)
fragment_fasta = config.get('fragment_fasta',
                            [])
# where to write ot fragments or where to find already clustered fragnents 
fragments_dir = config.get('fragments_dir', f'{working_dir}/fragments')

# final output fasta
output_file = config.get('output_fasta', f'{working_dir}/polished_fragments.fasta')

# use medaka
# medaka offers a slight improvement, but can be finicky. Skip it here.
USE_MEDAKA = config.get('use_medaka', True)

# if they user uses something non standard, treat it as False if it starts with an "f"
if isinstance(USE_MEDAKA, str):
    if USE_MEDAKA[0].lower == 'f':
      USE_MEDAKA = False
    else:
      USE_MEDAKA = True

# translating fragments to reads (if reading fragents from file)
# default fragmment namee is {readname}:{fragment_num}
fragment_pattern = config.get('fragment_pattern', r':\d+$')
fragment_rexp = re.compile(fragment_pattern)
fragment_repl = config.get('fragment_repl', '')
def get_read_from_fragment(frag):
    return fragment_rexp.sub(fragment_repl, frag)

RACON_ITERS = config.get('racon_iters', 3)
MIN_Q = config.get('min_q', 9)
MM_THREADS = config.get("mm_threads", 5)
POLISH_THREADS = config.get("polish_threads", 5)
if USE_MEDAKA:
    FINAL_NAME = 'medaka'
else:
    FINAL_NAME = f"draft.{RACON_ITERS}"

rule all:
    input:
        all_polished=output_file,

## optional starting points:
# If workgin from just the fragments, make a directory for each read with it's fragments in a fasta file
# If using the clustered fragments from Snakefile.frags, just use the existing fragments/ directory (no checkpoint)

if fragment_fasta:
    # only use checkpoint if we have to (working from. asingle fasta)
    checkpoint split_fragments_by_read:
        input: fragment_fasta
        output: directory(fragments_dir)
        run:
            # split input into fragments if provided
            for fragment in SeqIO.parse(str(input), 'fasta'):
                read = get_read_from_fragment(fragment.id)
                read_fasta = os.path.join(str(output), read, "all.fasta")
                if not os.path.exists(read_fasta):
                    os.makedirs(os.path.dirname(read_fasta))
                else:
                    # create a placeholder file to indecate that we got more than one frag for this read
                    with open(os.path.join(os.path.dirname(read_fasta),
                                           ".has_multiple_fragments"),
                              'wt') as dummy_out:
                        # just touch the file
                        pass
                with open(read_fasta, 'at') as fasta_out:
                    fasta_out.write(fragment.format('fasta'))

    def get_cluster_names():
        """ get the list of read names from the checkpoint output """
        output_dir = str(checkpoints.split_fragments_by_read.get().output)

        # simple glob will be faster for lots of files
        import glob

        # if we started with all read fragments, 
        # only take reads with multiple fragments
        # (the read is the full parent folder name)
        read_glob = f"{output_dir}/*/.has_multiple_fragments"

        # pull out the parent dir as read name for each file
        reads = [os.path.basename(os.path.dirname(hmf_file))
                 for hmf_file 
                 in glob.glob(read_glob)]

        return reads
else:
    # fragments dir already exists
    def get_cluster_names():
        """ get the list of read or cluster names from the checkpoint output """
        # simple glob will be faster for lots of files
        import glob

        # if we're given files, assume they are all good
        cluster_glob = f"{fragments_dir}/*/all.fasta"

        # pull out the parent dir as cluster name for each file with more than one fragment
        clusters = [os.path.basename(os.path.dirname(hmf_file))
                    for hmf_file 
                    in glob.glob(cluster_glob)
                    if sum(1 for r in SeqIO.parse(hmf_file, 'fasta')) > 1
                   ]

        return clusters
        
rule launch_self_polishing:
    """ This rule calls snakemake for a single cluster 
        
        Many clusters fail top polish, we'll assume they are false positive concatemers and 
        skip them later (checkpoint 2). For now, ignore the snakemake exit code and process
        all clusters. The checkpoint will clean them up.
    """
    input: '{prefix}/{cluster}/all.fasta'
    output:
        fasta=f'{{prefix}}/{{cluster}}/{FINAL_NAME}.fasta',
        lengths=f'{{prefix}}/{{cluster}}/{FINAL_NAME}.v.drafts.gene.lengths'
    log: '{prefix}/{cluster}/snake.self.polish.log'
    threads: POLISH_THREADS
    params:
        cluster_dir='{prefix}/{cluster}',
        snake_dir=os.path.abspath(workflow.basedir),
        output=f"{FINAL_NAME}.fasta {FINAL_NAME}.v.drafts.gene.lengths",
        conda=f"--use-conda --conda-frontend {workflow.conda_frontend}" \
              if workflow.use_conda else ""
    shell:
        """
        cd {params.cluster_dir}
        snakemake -s {params.snake_dir}/Snakefile.self_polish \
            {params.conda} \
            --config POLISH_THREADS={POLISH_THREADS} \
                     RACON_ITERS={RACON_ITERS} \
                     MM_THREADS={MM_THREADS} \
                     USE_MEDAKA={USE_MEDAKA} \
                     MIN_Q={MIN_Q} \
                     FINAL_NAME={FINAL_NAME} \
            -j {threads} -p -k \
            {params.output} \
          > snake.self.polish.log 2>&1 \
          || touch {params.output}
        """
    
## Checkpoint 2:
# drop any reads/clusters where the fragments didn't polish successfully

checkpoint drop_bad_clusters:
    input: lambda w: expand(f'{fragments_dir}/{{cluster}}/{FINAL_NAME}.fasta', \
                            cluster=get_cluster_names()),
    output: f'{working_dir}/good_fragment_clusters.list'
    run:
        with open(str(output), 'wt') as cluster_list:
            for input_file in input:
                if os.path.getsize(str(input_file)) > 0:
                    cluster = os.path.basename(os.path.dirname(str(input_file)))
                    cluster_list.write(f"{cluster}\n")
            # TODO: do some sanity checking:
            #  * make sure a paf file was generated
            #  * make sure the error is not just medaka (is there a racon output?)
            #  * report how many clusters got to each stage
                    

def get_good_cluster_names():
    """ get the list of cluster names with at least some self hits """
    cluster_list = str(checkpoints.drop_bad_clusters.get().output)
    
    with open(cluster_list) as lines:
        clusters = [l.strip() for l in lines]

    return clusters

rule collect_polished_fragments:
    """ collect each final VEIME from read/cluster subfolders into one final FASTA file """
    input: lambda w: expand(f'{fragments_dir}/{{cluster}}/{FINAL_NAME}.fasta', \
                            cluster=get_good_cluster_names()),
    output: output_file
    run:
        with open(str(output), 'wt') as fasta_out:
            for input_file in input:
                for fragment in SeqIO.parse(str(input_file), 'fasta'):
                    fasta_out.write(fragment.format('fasta'))


    
