"""
Given a set of fasta files and a table labelling them (see test/data/genome.sources.tsv)

And using:

 * collection of VOG HMMs
 * eggnog mapper
 
Annotate and cluster genomes

Outputs:

 * {WORKING_DIR}/{name}.metadata.tsv:           genomes with cluster and module asignments
 * {WORKING_DIR}/{name}.genes.tsv:              genome annotations
 * {WORKING_DIR}/{name}.bipartite.modules.json: module assignments for genes and genomes
 
"""
import os

######
# CONFIG
WORKING_DIR = config.get('working_dir', 'test/run')

# naming prefix for output files
NAME = config.get('name', 'genomes')
PREFIX = f"{WORKING_DIR}/{NAME}"

# eggnog annotations
# supply a location to use eggnog annots
EGGNOG_DATA_DIR = config.get('eggnog_data_dir', None)
if EGGNOG_DATA_DIR is not None:
        EGGNOG_DATA_DIR = os.path.abspath(EGGNOG_DATA_DIR)

#EGGNOG_DATA_DIR="/nvram/jmeppley/eggnog_mapper"
# set these to fast scratch disks for better performance (They can be the same
# or different)
EGGNOG_TMP_DIR = config.get('eggnog_tmp_dir', "./tmp")      # local paths are
                                                            # relative to
                                                            # WORKING_DIR
EGGNOG_SCRATCH_DIR = config.get('eggnog_scratch_dir', EGGNOG_TMP_DIR)

## minimum starting files

# HMM data for gene identification
hmm_db = config.get('hmm_db', 
                    'test/data/VOGS.hmm')

# dict from a source label to a fasta of genomes
genomes_source_file = config.get('genomes_source_file', f'test/data/genome.sources.tsv')

## other configuration
# collected genomes in one fasta file
genomes_fasta = config.get('genomes_fasta', f'{PREFIX}.fasta')
# dict from seq ID to source
genome_source_file = config.get('genome_source_file', f'{PREFIX}.sequence.sources.tsv')

#defaults
config.setdefault('hmmer_threads', 4)

wildcard_constraints:
    cutoff=r'(0|0?\.\d+)',
    mcl_i=r'(\d+|\.\d+|\d+\.\d*)',

mcl_template = "{prefix}.shared.gene.scores.c{cutoff}.I{mcl_i}.mcl"
cutoffs = [0,.25,.5]
mcl_is = [2,5]
mcl_outputs = expand("{prefix}.shared.gene.scores.c{cutoff}.I{mcl_i}.mcl",
                     cutoff=cutoffs, 
                     mcl_i=mcl_is,
                     prefix=[PREFIX,]
                    )

rule output:
    """ This workflow creates 6 MCL output files """
    input: 
        mcl_outputs,
        f"{PREFIX}.metadata.tsv"
        
rule collect_genomes:
    input: genomes_source_file
    output:
        fasta=genomes_fasta,
        source_map=genome_source_file
    run:
        import pandas
        from Bio import SeqIO
        
        # read fasta file names from source table and collect into one fasta
        count = 0
        source_dict = {}
        with open(str(output.fasta), 'wt') as fasta_out:
            for source, fasta_file in pandas.read_csv(str(input), sep='\t', 
                                                      usecols=[0,1], 
                                                     ).values:
                count += 1
                if count == 1 and not os.path.exists(fasta_file):
                    # the first line might just be a header
                    continue
                
                for seq in SeqIO.parse(fasta_file, 'fasta'):
                    if seq.id in source_dict:
                        raise Exception(f"Multiple sequences share n ID. Please rename!\nEG {seq.id}")
                    source_dict[seq.id] = source
                    fasta_out.write(seq.format('fasta'))
        
        # save map from sequence ID to source
        pandas.Series(source_dict, name='Source').to_csv(str(output.source_map), sep='\t')
            
rule prodigal:
    input: genomes_fasta
    output: 
        faa="{prefix}.prodigal.faa",
        gff="{prefix}.prodigal.gff",
    log: "{prefix}.prodigal.log"
    params:
        meta="-p meta" if config.get('prodigal_meta', True) else ""
    threads: config.get('prodigal_threads', 100)  # default is high because this is a bottleneck
    shell: """
        batch_launcher.py -N {threads} -K -v -X local -i -i -o -a -o -o \
            --  prodigal -i {input} -a {output.faa} -f gff -o {output.gff} \
                {params.meta} \
            > {log} 2>&1
        """
    
rule hmmer_batch:
    input:
        faa="{prefix}.prodigal.faa",
        hmm=hmm_db,
    output: "{prefix}.vs.HMMs.tbl"
    log: "{prefix}.vs.HMMs.tbl.log"
    threads: config.get('hmmer_batch_threads', 20)
    shell: """
        let N={threads}/{config[hmmer_threads]}
        echo running $N fragments of {config[hmmer_threads]} each
        batch_launcher.py -N $N -K -v -X local \
            --  hmmsearch --cpu {config[hmmer_threads]} --domtblout {output} -o /dev/null {input.hmm} {input.faa} \
            > {log} 2>&1
        """

rule hmmer_filter:
    """ condense tbl file into best hits """
    input: "{prefix}.vs.HMMs.tbl"
    output: "{prefix}.vs.HMMs.E.001.sort.F0.tbl"
    threads: 3
    shell: """
        filter_blast_m8.py {input} -f hmmsearchdom -E .001 \
             2> {output}.filter_1.log \
             | sort \
             | filter_blast_m8.py -f hmmsearchdom -s evalue -F 0  \
             > {output} \
             2> {output}.filter_2.log """
        

if EGGNOG_DATA_DIR is not None:
    rule eggnog_data:
        output: 
            dmnd=f'{EGGNOG_DATA_DIR}/eggnog_proteins.dmnd',
            tax=f'{EGGNOG_DATA_DIR}/eggnog.taxa.db',
            db=f'{EGGNOG_DATA_DIR}/eggnog.db'
        conda: "conda.eggnog.yaml"
        shell:
            "download_eggnog_data.py --data_dir {EGGNOG_DATA_DIR} -y"

    rule eggnog_annots:
        """ runs eggnog mapper on the gene predictions 
        
        Note: emapper seems to fail if the input/output files are not in the current dir
        """
        input:
            faa="{prefix}.prodigal.faa",
            db=f'{EGGNOG_DATA_DIR}/eggnog.db'
        output:
            annot="{prefix}.prodigal.emapper.annotations"
        log: "{prefix}.prodigal.emapper.log"
        params:
            faa=f"{NAME}.prodigal.faa",
            out_pref=f"{NAME}.prodigal"
        threads: config.get('emapper_threads', 40)
        conda: "conda.eggnog.yaml"
        shell:
            """
            cd {WORKING_DIR} && \
            mkdir -p {EGGNOG_TMP_DIR} && \
            mkdir -p {EGGNOG_SCRATCH_DIR} && \
            emapper.py \
                -i {params.faa} \
                --output {params.out_pref} \
                --temp_dir {EGGNOG_TMP_DIR} \
                --scratch_dir {EGGNOG_SCRATCH_DIR} \
                --cpu {threads} -m diamond \
                --data_dir {EGGNOG_DATA_DIR} \
                > {NAME}.prodigal.emapper.log 2>&1
            """
else:
    rule eggnog_annots:
        """ 
        Creates dummy eggnog mapper output file
        """
        input:
            faa="{prefix}.prodigal.faa"
        output:
            annot=touch("{prefix}.prodigal.emapper.annotations")


hlen_rexp = re.compile('\d+\s+VOG\d+\s+-\s+(\d+)')
def get_hlen(hit):
    return int(hlen_rexp.search(hit.line).group(1))

rule gene_annots:
    """ condense tbl file and eggnog annots into  gene_table """
    input: 
        faa="{prefix}.prodigal.faa",
        tbl= "{prefix}.vs.HMMs.E.001.sort.F0.tbl",
        eggnog="{prefix}.prodigal.emapper.annotations",
    output: "{prefix}.genes.tsv"
    run:
        import pandas
        # parse the eggnog mapper annotations file into a dict from gene to OG
        from jme.jupy_tools.utils import parse_eggnog_annotations
        # we only need the tax_og column
        no_eggnog = os.path.getsize(str(input.eggnog)) == 0
        if no_eggnog:
            eggnog_og_dict = {}
        else:
            eggnog_annots = parse_eggnog_annotations(str(input.eggnog))
            eggnog_og_dict = eggnog_annots['tax_og'].to_dict()

        # parse the gene names, starts, and ends from the faa file and add eggnog annotations
        from jme.jupy_tools.experimental.gene_synteny import import_genes
        gene_table = import_genes(str(input.faa),
                                  eggnog_og_dict)
        gene_table['eggnog'] = gene_table['annot']
        if no_eggnog:
            gene_table['eggnog_root'] = [None for g in gene_table.index]
        else:
            gene_table['eggnog_root'] = [eggnog_annots.root_og.get(g, None)
                                         for g in gene_table.index]

        # add hmm results to gene table, overwriting eggnog annot with VOG hit
        from edl.blastm8 import generate_hits, HMMSEARCHDOM
        
        # first pass: colect the new data:
        annot_details = {}
        for gene, hits in generate_hits(str(input.tbl),
                                        format=HMMSEARCHDOM, 
                                        evalue=.001):
            for hit in hits:
                vog = hit.hit
                vog_len = get_hlen(hit)
                h_frac = (abs(hit.hend - hit.hstart) + 1) / vog_len
                annot_details[gene] = \
                    (vog, vog, hit.hstart, hit.hend, vog_len, h_frac, hit.evalue)
                # just take the first hit
                break

        # update data frame all at once
        gene_table[['annot','vog','hmm_start', 'hmm_end', 'hmm_len', 'hmm_frac', 'evalue']] = \
            [annot_details.get(gene, (annot, None, None, None, None, None, None))
             for gene, annot in gene_table.annot.items()
            ]
        
        # alternate annot (using root eggNOG OG first and falling back to VOG)
        gene_table['annot_e'] = [r if pandas.notna(r) else v 
                                 for v,r in gene_table[['vog',
                                                        'eggnog_root']].values]
        

        # write out gene_table
        gene_table.to_csv(str(output), sep='\t')

## MCL clustering on shared gene ratio
rule calc_all_scores:
    """ build a score matrix based on shared gene pairs """
    input: "{prefix}.genes.tsv"
    output: "{prefix}.shared.gene.scores.c0.abc"
    run:
        # score each genome pair on how much of the annotated length is in shared gene families
        scores = {}

        # load gene data
        import pandas
        gene_table = pandas.read_csv(str(input), sep='\t', index_col=0)
        
        # collapse table into genome indexed with two columns: 
        #   1. dict of annot to total gene length for annot
        #   2. total gene length
        data = gene_table[['end','start','genome','annot']] \
            .eval('gene_len = end + 1 - start') \
            .groupby(['genome', 'annot']) \
            .agg({'gene_len':sum}) \
            .reset_index() \
            .set_index('annot') \
            [['genome','gene_len']] \
            .groupby('genome') \
            .agg({'gene_len':(dict, sum)}) \
            .reset_index() \
            .values

        # loop over pairs of genomes (and their annotation data)
        from itertools import combinations
        for genome_data_1, genome_data_2 in combinations(data, 2):
            (genome1, genes_lens_1, tot_len_1) = genome_data_1
            (genome2, genes_lens_2, tot_len_2) = genome_data_2
            
            # what genes are in both
            shared_genes = set(genes_lens_1).intersection(genes_lens_2)
            # what is the total length of these shared genes?
            tot_shared_gene_len = sum((genes_lens_1[g] + genes_lens_2[g] 
                                       for g in shared_genes if g != "Unknown"))
            # compare to total of all genes for score
            tot_gene_len = tot_len_1 + tot_len_2
            score = tot_shared_gene_len / tot_gene_len
            scores[(genome1, genome2)] = score
            scores[(genome2, genome1)] = score

        with open(str(output), 'wt') as abc_out:
            for (genome1, genome2), score in scores.items():
                abc_out.write(f"{genome1}\t{genome2}\t{score}\n")
    
rule filt_scores:
    input: "{prefix}.shared.gene.scores.c0.abc"
    output: "{prefix}.shared.gene.scores.c{cutoff}.abc"
    wildcard_constraints:
        cutoff=r'0\.\d+'
    run:
        cutoff = float(wildcards.cutoff)
        with open(str(output), 'wt') as scores_out:
            with open(str(input)) as scores_in:
                for line in scores_in:
                    g1, g2, score = line.rstrip().split('\t')
                    score = float(score)
                    if score >= cutoff:
                        scores_out.write(line)

ruleorder: calc_all_scores > filt_scores
        
rule mcl:
    input: "{root}.abc"
    output: "{root}.I{mcl_i}.mcl"
    threads: 10
    shell: "mcl {input} --abc -te {threads} -I {wildcards.mcl_i} -o {output} > {output}.log 2>&1"

## Bipartite modules
rule bipartite_modules:
    input: "{prefix}.genes.tsv"
    output: "{prefix}.bipartite.modules.json"
    conda: "conda.modules.yaml"
    script: "bipartite_modules.py"
        
rule summarize_genomes:
    """ make a table of genome meatadata for quick access """
    input: 
        genes="{prefix}.genes.tsv",
        fasta=genomes_fasta,
        clusters=mcl_outputs,
        modules=rules.bipartite_modules.output,
        sources=genome_source_file,
    output:
        "{prefix}.metadata.tsv"
    run:
        import pandas, numpy
        from Bio import SeqIO
        
        # load data
        gene_table = pandas.read_csv(str(input.genes), sep='\t', index_col=0)
        seq_sources = pandas.read_csv(str(input.sources), sep='\t', index_col=0)
        ref_lens = {g.id:len(g) for g in SeqIO.parse(str(input.fasta), 'fasta')}
        clusters_by_params = {}
        for mcl_file in input.clusters:
            mcl_params = "_".join(mcl_file.split(".")[-3:-1])
            clusters = clusters_by_params.setdefault(mcl_params, [])
            with open(mcl_file) as lines:
                for line in lines:
                    clusters.append([s.strip() for s in line.split('\t')])

        # collapse gene table by genome (adding length and source)
        genome_df = \
            gene_table \
                .groupby('genome') \
                .agg({'gene_no':len}) \
                .rename({'gene_no': 'Gene_count'}, axis=1)
        
        genome_df = genome_df \
                .join(pandas.Series(ref_lens, name='Length')) 
        
        genome_df = genome_df \
                .join(seq_sources)

        # add clsuter info
        for params, clusters in clusters_by_params.items():
            param_label = f"mcl_{params}"
            genome_to_cluster_id = {g:i 
                                    for i, cluster in enumerate(clusters) 
                                    for g in cluster}
            genome_df[param_label] = numpy.array(
                [genome_to_cluster_id.get(g, -1) for g in genome_df.index],
                dtype=int,
            )
            
        # add module info
        with open(str(input.modules)) as json_in:
            module_data = json.load(json_in)['membership_by_resolution']
        for resolution, module_assignments in module_data.items():
            col_key = f'module_{resolution}'
            genome_df[col_key] = numpy.array(
                [module_assignments.get(g, -1) for g in genome_df.index],
            )
            
        # save table
        genome_df.to_csv(str(output), sep='\t')
